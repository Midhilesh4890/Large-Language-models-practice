{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOXiBx3GCiza0U4j7kUNpye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Midhilesh4890/Large-Language-models-practice/blob/main/Mercor_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "b0M4BzWBSF5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "cFhep_udSJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywq9_hEg-JH5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers python-dotenv\n",
        "!pip install huggingface_hub --upgrade\n",
        "!apt-get install wget coreutils\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/meta-llama/llama3.git"
      ],
      "metadata": {
        "id": "fdBkaL0U-ibV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama3\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "F0yoAqF0-R3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x download.sh"
      ],
      "metadata": {
        "id": "I9V2JQdkXcU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the URL and model name in a variable\n",
        "url = \"https://download6.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiOXhsbTlhd3czbjAwNDJxc3NsbjFyNmIzIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQ2LmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3MTY5MjEzMDJ9fX1dfQ__&Signature=E%7E6anzNIlvIol5R8JXT4jLP0rcIXkKPg%7E6jEFwzEr3tGMAyK9LCOtvXHlUGUO0pOCRf3-Jw%7EdIl3j30lY%7ELYmWP77NnoyLKIJDcQgRByi01vblVy3OnoQSyFzA-M5fAhPfzJ%7EH7v2aLLo433%7ErqIKEu1mnYVee%7EDeU%7EytJbMNsjHp3BTBzvrJ98e89y%7EtMQB9iXr5acMxoOj19WG4RReworhYSO7HnTWfDvAsOfSwEjgY6EpUJ2AVpfxo2H5EDookDFOdeT4EfCbhMSGDE-sJeBp6ERwJwFAzBmEHRKgKdfZ0o-PKm0gC0qjctWGCsJfSSXPFSWPjZ%7EGsJ25VB7rWQ__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=545877497846408\"\n",
        "model_name = \"8B,8B-instruct\"\n",
        "\n",
        "# Download the model using the !download.sh script\n",
        "!./download.sh \"$url\" \"{model_name}\""
      ],
      "metadata": {
        "id": "RzuepDEVfNlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from dotenv import load_dotenv\n",
        "# from huggingface_hub import HfApi, notebook_login\n",
        "\n",
        "# load_dotenv(\".env\")\n",
        "# HFTOKEN = os.environ.get(\"HFTOKEN\")\n",
        "# notebook_login(HFTOKEN)\n",
        "# !git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct /content/meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "id": "W6otFbDYbvQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/llama3"
      ],
      "metadata": {
        "id": "qFPmxnnqXk3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !mv /content/meta-llama/Meta-Llama-3-8B-Instruct /content/drive/LLAMA3/\n"
      ],
      "metadata": {
        "id": "0dlFwx3kXl09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xNteQPy7C7ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizer"
      ],
      "metadata": {
        "id": "ABbqyJviTHsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class RMSNorm(nn.Module):\n",
        "#     def __init__(self, d_model, eps=1e-8):\n",
        "#         super(RMSNorm, self).__init__()\n",
        "#         self.eps = eps\n",
        "#         self.scale = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
        "#         logging.basicConfig(filename='rmsnorm.log', level=logging.INFO)\n",
        "#         logging.info(f\"RMSNorm value: {rms.mean().item()}\")\n",
        "#         return x * self.scale / rms\n",
        "\n",
        "# Define RMSNorm class with logging\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, layer_name, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = torch.nn.Parameter(torch.ones(dim))\n",
        "        self.layer_name = layer_name\n",
        "        self.call_count = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.call_count += 1\n",
        "        if self.call_count <= 65:\n",
        "            rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
        "            logging.info(f'Layer: {self.layer_name}, Call: {self.call_count}, RMS: {rms}')\n",
        "        return x / rms * self.scale"
      ],
      "metadata": {
        "id": "VKu_0lj4C8WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "1TyU6vR-ErMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct the import path to the model\n",
        "from llama import LlamaForCausalLM\n",
        "\n",
        "def load_pretrained_model(model_name='llama-3-8B'):\n",
        "    model = LlamaModel.from_pretrained(model_name)\n",
        "    # Replace existing normalization with the modified RMSNorm if needed\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.LayerNorm):  # Assuming LayerNorm needs to be replaced\n",
        "            setattr(model, name, RMSNorm(module.normalized_shape[0]))\n",
        "    return model\n",
        "\n",
        "model = load_pretrained_model()\n",
        "model.to(torch.float16)  # Set model to BF16 precision"
      ],
      "metadata": {
        "id": "b9ib7LVIdoW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Example: loop through the dataset and compute accuracy\n",
        "        for inputs, targets in dataset:\n",
        "            predictions = model(inputs)\n",
        "            # Compute accuracy logic here\n",
        "    return accuracy\n",
        "\n",
        "# Assuming dataset loading and preparation is done\n",
        "accuracy = evaluate_model(model, dataset)\n",
        "print(f\"Model accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "5dLtftHKEtlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zoyPoRHCOw5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load logged RMS values\n",
        "rms_values = np.loadtxt('rmsnorm.log', usecols=[-1])\n",
        "\n",
        "# Plot histogram\n",
        "sns.histplot(rms_values, kde=True)\n",
        "plt.title(\"Histogram of RMSNorm values\")\n",
        "plt.xlabel(\"RMSNorm Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gP0vXkVCEwkA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}