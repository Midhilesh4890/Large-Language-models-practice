{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mv6r8SQ0CRvugFiXU/7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Midhilesh4890/Large-Language-models-practice/blob/main/NER_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Ctq1PXlTpwg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_data(file_path):\n",
        "    ner_data = pd.read_csv(file_path, encoding='latin1')\n",
        "    ner_data['Sentence #'].fillna(method='ffill', inplace=True)\n",
        "    grouped_data = ner_data.groupby('Sentence #').apply(lambda s: [(w, t) for w, t in zip(s['Word'].values.tolist(), s['Tag'].values.tolist())])\n",
        "    return [sentence for sentence in grouped_data]\n",
        "\n",
        "def create_mappings(sentences):\n",
        "    words = [word for sentence in sentences for word, tag in sentence]\n",
        "    tags = [tag for sentence in sentences for word, tag in sentence]\n",
        "    word2idx = {w: i + 1 for i, w in enumerate(set(words))}\n",
        "    tag2idx = {t: i for i, t in enumerate(set(tags))}\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "def process_data(sentences, word2idx, tag2idx, max_len):\n",
        "    X = [[word2idx.get(w[0], 0) for w in s] for s in sentences]\n",
        "    X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0)\n",
        "\n",
        "    y = [[tag2idx.get(w[1], 0) for w in s] for s in sentences]\n",
        "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx.get(\"O\", 0))\n",
        "    y = np.array([to_categorical(i, num_classes=len(tag2idx)) for i in y])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def build_model(word2idx, tag2idx, lstm_units, dense_units):\n",
        "    max_len = 50\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "    embedding_layer = Embedding(input_dim=len(word2idx) + 1, output_dim=50, input_length=max_len)(input_layer)\n",
        "    lstm_layer = LSTM(units=lstm_units, return_sequences=True)(embedding_layer)\n",
        "    dropout_layer = Dropout(0.1)(lstm_layer)\n",
        "    dense_layer = Dense(dense_units, activation='relu')(dropout_layer)\n",
        "    output_layer = Dense(len(tag2idx), activation='softmax')(dense_layer)\n",
        "    model = Model(input_layer, output_layer)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "T77jES65W2Jm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script\n",
        "file_path = 'ner_dataset.csv'\n",
        "sentences = load_and_prepare_data(file_path)\n",
        "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
        "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.25, random_state=42)\n",
        "\n",
        "word2idx, tag2idx = create_mappings(train_sentences)\n",
        "X_train, y_train = process_data(train_sentences, word2idx, tag2idx, max_len=50)\n",
        "X_test, y_test = process_data(test_sentences, word2idx, tag2idx, max_len=50)\n",
        "\n",
        "# Define the model configurations in a list of tuples\n",
        "model_configs = [(64, 32), (128, 64), (256, 128)]\n",
        "\n",
        "# Initialize, train, and save each model in a single loop\n",
        "predictions = []\n",
        "for i, (lstm_units, dense_units) in enumerate(model_configs, start=1):\n",
        "    model = build_model(word2idx, tag2idx, lstm_units, dense_units)\n",
        "    model.fit(X_train, y_train, batch_size=32, epochs=1, validation_split=0.1)\n",
        "    predictions.append(model.predict(X_test, verbose=1))\n",
        "    model.save_weights(f'ner_model_{i+1}.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSs88oOKT3Up",
        "outputId": "305d84c3-b3f1-4cee-f4e5-0a11f848a1f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "810/810 [==============================] - 41s 48ms/step - loss: 0.2999 - accuracy: 0.9400 - val_loss: 0.1292 - val_accuracy: 0.9623\n",
            "300/300 [==============================] - 3s 10ms/step\n",
            "810/810 [==============================] - 77s 91ms/step - loss: 0.2497 - accuracy: 0.9471 - val_loss: 0.1182 - val_accuracy: 0.9643\n",
            "300/300 [==============================] - 8s 24ms/step\n",
            "810/810 [==============================] - 145s 177ms/step - loss: 0.2256 - accuracy: 0.9513 - val_loss: 0.1016 - val_accuracy: 0.9711\n",
            "300/300 [==============================] - 14s 46ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_predictions = np.mean(np.array(predictions), axis=0)"
      ],
      "metadata": {
        "id": "dPvZHE_IT8c6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the predictions and true values to label sequences\n",
        "pred_labels = np.argmax(ensemble_predictions, axis=-1)\n",
        "true_labels = np.argmax(y_test, axis=-1)\n",
        "\n",
        "# Convert indices to tags\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "pred_tags = [[idx2tag[i] for i in row] for row in pred_labels]\n",
        "true_tags = [[idx2tag[i] for i in row] for row in true_labels]"
      ],
      "metadata": {
        "id": "go07ipV1VUhl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over all sentences in the test set\n",
        "for i, (sentence, true, pred) in enumerate(zip(test_sentences, true_tags, pred_tags)):\n",
        "    for word, true_tag, pred_tag in zip(sentence, true, pred):\n",
        "        # Add each word, its true tag, and its predicted tag to the results list\n",
        "        results.append({\"Word\": word[0], \"True_Tag\": true_tag, \"Pred_Tag\": pred_tag})\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv(\"ner_results.csv\", index=False)\n",
        "\n",
        "# Print a message to indicate completion\n",
        "print(\"Results saved to ner_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34fN02NqVVIO",
        "outputId": "1e51e124-708e-4234-9f39-4cf96b9c5a36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to ner_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.read_csv('ner_results.csv')\n",
        "result_df['Pred_Tag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgxlyNesVY2X",
        "outputId": "2f521528-2f71-4539-c7f3-f9753f6f1d87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O        182629\n",
              "B-geo     10188\n",
              "I-per      3262\n",
              "B-tim      3192\n",
              "I-org      2787\n",
              "B-per      2617\n",
              "B-gpe      2458\n",
              "B-org      1472\n",
              "I-geo       958\n",
              "I-tim       139\n",
              "Name: Pred_Tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}